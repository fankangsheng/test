# 会议纪要

**会议主题**：强化学习  
**会议时间**：2025年6月3日，下午 16:00-18:00  
**会议地点**：创新楼207  
**出席人员**：唐于涛、胡崇源、徐成志、张一凡、张燚辰、孟帅明、樊康生、孙啟铭、徐嘉瑄

## 会议内容

### 主讲人环节

#### 主要内容
张一凡介绍了强化学习（Reinforcement Learning, RL）的基础知识与前沿进展。主要内容包括以下几个方面：
- 基础概念：阐述了强化学习与监督学习的区别，介绍策略、价值函数和模型的定义，结合马尔可夫奖励过程和马尔可夫决策过程解释智能体在环境中如何做出决策。
- 经典算法：讲解了蒙特卡洛法、时序差分方法、Q-learning、策略梯度、近端策略优化（PPO）及Actor-Critic结构。
- 多智能体强化学习：集中式训练、分布式执行的架构
- RLHF: PPO、DPO、GRPO等。

#### 问题讨论

1. 状态价值函数是否与时间t相关
- 如果环境不是时变的，则与时间t无关；如果环境本身是变化的，那么你在不同时间，同一状态，做同样的动作，得到的奖励可能是不一样的
2. 关于策略选择
- 随机策略或者是确定性策略或其他的策略形式，实际上是对智能体的假设，往往认为智能体是有限理性的。
3. 为什么策略梯度计算时要将 $p_\theta$ 写成 $logp_\theta$ 
- 对 $p_\theta$ 求导是一个概率连乘的形式，而对 $logp_\theta$ 求导是一个累加的形式，更简单。
4. 重要性采样的公式表述不准确，整体讲解内容不够严谨


